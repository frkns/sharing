{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b8761f4-971e-4482-8514-782ee9453069",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium.wrappers import FrameStackObservation\n",
    "from IPython.display import display, clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import gymnasium as gym\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import random\n",
    "import ale_py\n",
    "import time\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ebb80ec-d6fe-49c1-86f9-2802f5c5a644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0.0\n",
      "2.6.0+cpu\n",
      "10\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "print(gym.__version__)\n",
    "print(torch.__version__)\n",
    "print(torch.get_num_threads())\n",
    "print(torch.get_num_interop_threads())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba0e5c3a-6eb9-457b-8fa1-36c235423bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Training on device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5924082b-8eab-4f15-9c16-154fc44fcc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remake_env(render_mode=None):\n",
    "    global env\n",
    "    if 'env' in globals(): \n",
    "        env.close()\n",
    "        del env\n",
    "    env = gym.make('BreakoutDeterministic-v4', render_mode=render_mode)\n",
    "    env = FrameStackObservation(env, 4)  # stack previous 4 frames to simulate motion and mend Markov property"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ba53780-0bc1-4c86-a282-d2e293377379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters from Nature (in the same order shown in the paper, just different names)\n",
    "# 'action repeat' not included because it's handled by env frameskip\n",
    "# RMSProp gradient parameters not included because we're using AdamW\n",
    "batch_size = 32\n",
    "memory = ReplayBuffer(100_000)  # holding 1 million requires ~4*84*84*2*1e6 ~ 56 gb of memory so i'll use 100k instead - hopefully it's good\n",
    "sync_freq = 10_000  # environment steps\n",
    "gamma = 0.99\n",
    "learn_freq = 4\n",
    "learning_rate = 0.00025\n",
    "eps_max = 1.0  # initial epsilon\n",
    "eps_min = 0.1  # final\n",
    "eps_anneal_steps = 1_000_000\n",
    "learning_starts = 50_000  # uniform random policy is run for X steps before learning starts\n",
    "noop_max = 30  # note: for breakout we should use action=1 (FIRE) instead of 0 so the ball releases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f684fdb-42a8-4b33-89e1-26a2388e5add",
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_clip = (-1, 1)\n",
    "max_steps = 108000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2bcc63eb-8206-4585-8ed3-e96a212dc501",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_epsilon():  # epsilon schedule\n",
    "    effective_steps = step_count - 14433239\n",
    "    return max(eps_min, eps_min + (eps_max - eps_min) * (1 - effective_steps / eps_anneal_steps))  # linear annealing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3827101b-965b-4bcf-b418-74b84c232f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "online_net = QNetwork()\n",
    "target_net = QNetwork()\n",
    "target_net.eval()\n",
    "\n",
    "online_net.to(device)\n",
    "target_net.to(device)\n",
    "\n",
    "def sync(): \n",
    "    target_net.load_state_dict(online_net.state_dict())\n",
    "    \n",
    "sync()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70b21740-310b-44a3-b0e0-2740a69df15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(online_net.parameters(), lr=learning_rate)  # probably fine not to load optimizer state\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "196d5cee-0547-4071-9465-1d499c085738",
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_count = 0\n",
    "step_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "acf48331-440e-42fd-bb9f-b4979c7c5f51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded\n"
     ]
    }
   ],
   "source": [
    "# load latest checkpoint file if there is one\n",
    "proj_name = 'breakout_feb_27'\n",
    "\n",
    "checkpoint_file, checkpoint_version = get_checkpoint()\n",
    "# checkpoint_version = 12\n",
    "if True:\n",
    "    checkpoint = torch.load('./checkpoints/' + checkpoint_file, weights_only=False, map_location=torch.device(device))\n",
    "    online_net.load_state_dict(checkpoint['online_state_dict'])\n",
    "    target_net.load_state_dict(checkpoint['target_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    step_count = checkpoint.get('step_count')\n",
    "    episode_count = checkpoint.get('episode_count')\n",
    "    memory = ReplayBuffer(100_000)\n",
    "    try:\n",
    "        memory = torch.load('./checkpoints/mem-' + checkpoint_file, weights_only=False)\n",
    "    except:\n",
    "        pass\n",
    "    print('loaded')\n",
    "# else:\n",
    "#     checkpoint_file = str(checkpoint_version) + '-' + proj_name\n",
    "\n",
    "prev_mem_name = '--sentinel--'\n",
    "def save_checkpoint():\n",
    "    global prev_mem_name\n",
    "    name = f'./checkpoints/{checkpoint_version}-{proj_name}-{episode_count}e-{step_count}s.pth'\n",
    "    mem_name = f'./checkpoints/mem-{checkpoint_version}-{proj_name}-{episode_count}e-{step_count}s.pth'\n",
    "    if os.path.exists(prev_mem_name):\n",
    "        os.remove(prev_mem_name)\n",
    "    prev_mem_name = mem_name\n",
    "    checkpoint = {\n",
    "        'online_state_dict': online_net.state_dict(),\n",
    "        'target_state_dict': target_net.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'step_count': step_count,\n",
    "        'episode_count': episode_count\n",
    "    }\n",
    "    torch.save(checkpoint, name)\n",
    "    torch.save(memory, mem_name)  # don't save here to save space on kaggle\n",
    "    print('saved', name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0d7d5716-e8d7-4b58-95c8-9a2cc8f93ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy(preprocessed_state):\n",
    "    return online_net(preprocessed_state).argmax().item()\n",
    "\n",
    "def epsilon_greedy(preprocessed_state):\n",
    "    if np.random.random() < get_epsilon():\n",
    "        return env.action_space.sample()\n",
    "    return greedy(preprocessed_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "170f7b22-1dc7-44a0-9af2-ed1dd15f7cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_action(action, update_memory=True):\n",
    "    global state\n",
    "    next_state, reward, terminated, truncated, info = env.step(action)\n",
    "    next_state = preprocess_state(next_state)\n",
    "    if update_memory:\n",
    "        memory.push(state, action, reward, next_state, terminated)\n",
    "    state = next_state\n",
    "    reward = np.clip(reward, *reward_clip)  # wrong position for clip!!\n",
    "    return reward, terminated, truncated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c7c26b3b-668a-4b28-9893-08d4a0009abe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint 61-breakout_feb_27-51126e-14386023s.pth\n",
      "checkpoint version 61\n",
      "episode_count 51126\n",
      "step_count 14386023\n"
     ]
    }
   ],
   "source": [
    "print(f'checkpoint {checkpoint_file}')\n",
    "print(f'checkpoint version {checkpoint_version}')\n",
    "print(f'episode_count {episode_count}')\n",
    "print(f'step_count {step_count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d53435ae-527f-4b55-9109-b107ed602bf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 191\t(0.1/s)  [total 51317]\n",
      "step 47216\t(34/s)  [total 14433239]\n",
      "time 1404.47 s\n",
      "---\n",
      "avg. return: 1.80000  (last 100 episodes)\n",
      "epsilon 0.10000\n",
      "\u001b[31m Interrupt signal detected \u001b[32m training state is saved as checkpoint \u001b[0m\n",
      "saved ./checkpoints/62-breakout_feb_27-51322e-14434276s.pth\n"
     ]
    }
   ],
   "source": [
    "train_episodes = 100_000\n",
    "\n",
    "writer = SummaryWriter(log_dir='./runs/1st-run')\n",
    "return_history = []\n",
    "remake_env(None)\n",
    "t0 = time.time()\n",
    "start_episode, start_step = episode_count, step_count\n",
    "try:\n",
    "    while len(memory) < learning_starts:\n",
    "    # while len(memory) < 50:\n",
    "        state, info = env.reset()\n",
    "        state = preprocess_state(state)\n",
    "        for step in range(max_steps):\n",
    "            reward, terminated, truncated = do_action(env.action_space.sample())\n",
    "            if len(memory) % 100 == 0:\n",
    "                clear_output(wait=True)\n",
    "                print(f'collecting initial training samples {len(memory)}/{learning_starts}')\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "                    \n",
    "    for episode in range(train_episodes):\n",
    "        episode_return = 0\n",
    "        state, info = env.reset()\n",
    "        episode_count += 1\n",
    "    \n",
    "        for noop_step in range(np.random.randint(0, noop_max)):  # no-op start\n",
    "            state, reward, terminated, truncated, info = env.step(1)\n",
    "            if terminated or truncated:\n",
    "                print('noop start ended episode? probably should not be happening')\n",
    "                with open('_warnings.txt', 'w') as f:\n",
    "                    f.write(f'noop start ended episode @ step {step_count}')\n",
    "                state, info = env.reset()\n",
    "                break\n",
    "        state = preprocess_state(state)  # make sure to preprocess before running do_action\n",
    "\n",
    "        for step in range(max_steps):  # step\n",
    "            action = epsilon_greedy(state)\n",
    "            reward, terminated, truncated = do_action(action)\n",
    "            episode_return += reward\n",
    "            step_count += 1\n",
    "    \n",
    "            if step_count % sync_freq == 0:  # update target net\n",
    "                sync()\n",
    "            \n",
    "            if step_count % learn_freq == 0:  # update online net\n",
    "                states, actions, rewards, next_states, terminateds = memory.sample(batch_size)\n",
    "                # note: states and next_states should be kept uint8\n",
    "                # states = torch.tensor(states, dtype=torch.float, device=device)                           # (m, 4, 84, 84), m = batch_size\n",
    "                actions = torch.tensor(actions, dtype=torch.long, device=device).reshape(-1, 1)           # (m, 1)\n",
    "                rewards = torch.tensor(rewards, dtype=torch.float, device=device).reshape(-1, 1)          # (m, 1)\n",
    "                # next_states = torch.tensor(next_states, dtype=torch.float, device=device)                 # (m, 4, 84, 84)\n",
    "                terminateds = torch.tensor(terminateds, dtype=torch.float, device=device).reshape(-1, 1)  # (m, 1)\n",
    "                \n",
    "                pred = online_net(states).gather(1, actions)  # predicted Q-values of the selected action\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    y = rewards + gamma * target_net(next_states).max(axis=1, keepdim=True).values * (1 - terminateds)\n",
    "                \n",
    "                loss = criterion(pred, y)  # don't need to detach but y.requires_grad is False\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                max_grad = max(p.grad.abs().max().item() for p in online_net.parameters() if p.grad is not None)\n",
    "                total_norm = torch.nn.utils.clip_grad_norm_(online_net.parameters(), float('inf'))\n",
    "                writer.add_scalar('loss', loss.item(), step_count)\n",
    "                writer.add_scalar('max_grad', max_grad, step_count)\n",
    "                writer.add_scalar('total_norm', total_norm, step_count)\n",
    "                optimizer.step()\n",
    "            \n",
    "            if terminated or truncated:\n",
    "                break\n",
    "\n",
    "        writer.add_scalar('episode_steps', step, step_count)\n",
    "        writer.add_scalar('episode_return', episode_return, step_count)\n",
    "        writer.add_scalar('epsilon', get_epsilon(), step_count)\n",
    "        return_history.append(episode_return)\n",
    "    \n",
    "        if episode % 1000 == 0 and episode != 0:\n",
    "            checkpoint_version += 1\n",
    "            save_checkpoint()\n",
    "            writer.flush()\n",
    "    \n",
    "        if episode % 10 == 0 or episode == train_episodes-1:\n",
    "            tt = time.time() - t0\n",
    "            et = episode_count - start_episode \n",
    "            st = step_count - start_step \n",
    "            # plt.plot(return_history)\n",
    "            \n",
    "            clear_output(wait=True)\n",
    "            print(f'episode {et}\\t({et/tt:.1f}/s)  [total {episode_count}]')\n",
    "            print(f'step {st}\\t({st/tt:.0f}/s)  [total {step_count}]')\n",
    "            print(f'time {tt:.2f} s')\n",
    "            print('---')\n",
    "            print(f'avg. return: {np.mean(return_history[-100:]):.5f}  (last 100 episodes)')\n",
    "            print(f'epsilon {get_epsilon():.5f}')\n",
    "            # plt.show()\n",
    "            \n",
    "except KeyboardInterrupt:\n",
    "    print('\\033[31m Interrupt signal detected \\033[32m training state is saved as checkpoint \\033[0m')\n",
    "finally:\n",
    "    checkpoint_version += 1\n",
    "    save_checkpoint()\n",
    "    writer.close()\n",
    "    \n",
    "    \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872270d6-2672-4555-a665-0b2220141751",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ceb99de7-39a9-4774-b6ae-6d3f1515af81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmarking Summary\n",
    "# 424 env steps/s, ::2 downsample + normalization + torch.tensor\n",
    "# 46 passes/s, with AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b03c8d77-6457-42d4-97eb-f9cb070e2d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n = QNetwork()\n",
    "# t0 = time.time()\n",
    "# for k in range(1001):\n",
    "#     pred = n(torch.rand(1, 4, 84, 84))\n",
    "#     if k % 50 == 0:\n",
    "#         t = time.time() - t0\n",
    "#         clear_output(wait=True)\n",
    "#         print(f'{k} forward passes completed in {t:.2f} s  ({k/t :.2f}/s)')\n",
    "\n",
    "# 326 forwards/s        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5dd9af3b-aea6-45e7-9527-96428b6f139b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n = QNetwork()\n",
    "# x = preprocess_state(state)\n",
    "# optimizer = torch.optim.AdamW(n.parameters(), lr=0.001)\n",
    "# t0 = time.time()\n",
    "# for k in range(1001):\n",
    "#     l = (n(torch.rand(1, 4, 84, 84)) + k).sum()\n",
    "#     optimizer.zero_grad()\n",
    "#     l.backward(retain_graph=True)\n",
    "#     optimizer.step()\n",
    "#     if k % 50 == 0:\n",
    "#         t = time.time() - t0\n",
    "#         clear_output(wait=True)\n",
    "#         print(f'{k} backward passes completed in {t:.2f} s  ({k/t :.2f}/s)')\n",
    "\n",
    "# 146 backwards/s, no forward pass, no optimizer step\n",
    "# 46 backwards/s, with forward pass and AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "12b98d4e-f05a-4633-afdd-fd4817bb98b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# benchmark env time vs network inference time\n",
    "# remake_env(None)\n",
    "# steps = 0\n",
    "# t0 = time.time()\n",
    "# for episode in range(100):\n",
    "#     state, info = env.reset()\n",
    "#     while True:\n",
    "#         steps += 1\n",
    "#         # action = env.action_space.sample()\n",
    "#         action = 1\n",
    "#         state, reward, done, truncated, info = env.step(action)\n",
    "#         _ = preprocess_state(state)\n",
    "#         if done or truncated:\n",
    "#             break\n",
    "#         if steps % 1000 == 0:\n",
    "#             t = time.time() - t0\n",
    "#             clear_output(wait=True)\n",
    "#             print(f'{episode} episodes, {steps} steps ({steps/t :.2f}/s)')\n",
    "# t = time.time() - t0\n",
    "# print(f'{steps} steps completed in {t:.2f}  ({steps/t :.2f}/s)')\n",
    "\n",
    "# 676 steps/s, no preprocessing\n",
    "# 117 steps/s, np.dot graysacle, no cv2 resize\n",
    "# 111 steps/s, np.dot grayscale and cv2 resize\n",
    "# 566 steps/s, cv2 grayscale and cv2 resize (!)\n",
    "# 396 steps/s, cv2 grayscale and cv2 resize + normalization + torch.tensor\n",
    "# 424 steps/s, ::2 downsample + normalization + torch.tensor"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggleenv310",
   "language": "python",
   "name": "kaggle310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
