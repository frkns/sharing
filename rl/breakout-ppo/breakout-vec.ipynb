{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T04:34:09.533963Z",
     "iopub.status.busy": "2025-03-23T04:34:09.533759Z",
     "iopub.status.idle": "2025-03-23T04:34:16.299748Z",
     "shell.execute_reply": "2025-03-23T04:34:16.298607Z",
     "shell.execute_reply.started": "2025-03-23T04:34:09.533920Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "KAGGLE = 'KAGGLE_KERNEL_RUN_TYPE' in os.environ  # detect if notebook is running on Kaggle\n",
    "if KAGGLE:\n",
    "    !pip install gymnasium[atari]==1.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T04:34:16.301732Z",
     "iopub.status.busy": "2025-03-23T04:34:16.301412Z",
     "iopub.status.idle": "2025-03-23T04:34:23.777249Z",
     "shell.execute_reply": "2025-03-23T04:34:23.776569Z",
     "shell.execute_reply.started": "2025-03-23T04:34:16.301701Z"
    }
   },
   "outputs": [],
   "source": [
    "from gymnasium.wrappers import Autoreset, TimeAwareObservation, FrameStackObservation, AtariPreprocessing, ClipReward\n",
    "from collections import defaultdict, deque\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import random\n",
    "import ale_py\n",
    "import wandb\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T04:34:23.779183Z",
     "iopub.status.busy": "2025-03-23T04:34:23.778760Z",
     "iopub.status.idle": "2025-03-23T04:34:23.784267Z",
     "shell.execute_reply": "2025-03-23T04:34:23.783442Z",
     "shell.execute_reply.started": "2025-03-23T04:34:23.779161Z"
    }
   },
   "outputs": [],
   "source": [
    "print(gym.__version__, torch.__version__, torch.get_num_threads(), torch.get_num_interop_threads(), sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T04:34:23.785924Z",
     "iopub.status.busy": "2025-03-23T04:34:23.785625Z",
     "iopub.status.idle": "2025-03-23T04:34:23.881359Z",
     "shell.execute_reply": "2025-03-23T04:34:23.880672Z",
     "shell.execute_reply.started": "2025-03-23T04:34:23.785902Z"
    }
   },
   "outputs": [],
   "source": [
    "# globals\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "env_name = 'BreakoutNoFrameskip-v4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T04:34:23.882524Z",
     "iopub.status.busy": "2025-03-23T04:34:23.882293Z",
     "iopub.status.idle": "2025-03-23T04:34:23.897359Z",
     "shell.execute_reply": "2025-03-23T04:34:23.896544Z",
     "shell.execute_reply.started": "2025-03-23T04:34:23.882504Z"
    }
   },
   "outputs": [],
   "source": [
    "def make_env(render_mode=None):\n",
    "    env = gym.make(env_name, render_mode=render_mode, max_episode_steps=10_000)\n",
    "    # env = TimeAwareObservation(env)\n",
    "    env = Autoreset(env)  # TimeAwareObservation before Autoreset or else timestep tracking will be erroneous!!\n",
    "    env = AtariPreprocessing(env)\n",
    "    env = FrameStackObservation(env, 4)\n",
    "    env = ClipReward(env, -1, 1)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T04:34:23.898414Z",
     "iopub.status.busy": "2025-03-23T04:34:23.898200Z",
     "iopub.status.idle": "2025-03-23T04:34:23.913004Z",
     "shell.execute_reply": "2025-03-23T04:34:23.912396Z",
     "shell.execute_reply.started": "2025-03-23T04:34:23.898395Z"
    }
   },
   "outputs": [],
   "source": [
    "def make_envs(n_envs=32):  # >32 might cause memory issues\n",
    "    envs = gym.make_vec(\n",
    "        env_name, \n",
    "        max_episode_steps=10_000,\n",
    "        num_envs=n_envs, \n",
    "        vectorization_mode='async', \n",
    "        vector_kwargs={\n",
    "            # 'autoreset_mode': gym.vector.AutoresetMode.DISABLED,\n",
    "        },\n",
    "        wrappers=[  # make_vec autoresets by default\n",
    "            AtariPreprocessing, \n",
    "            lambda env: FrameStackObservation(env, 4),\n",
    "            lambda env: ClipReward(env, -1, 1),\n",
    "        ]\n",
    "    )\n",
    "    return envs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T04:34:23.913923Z",
     "iopub.status.busy": "2025-03-23T04:34:23.913731Z",
     "iopub.status.idle": "2025-03-23T04:34:23.928608Z",
     "shell.execute_reply": "2025-03-23T04:34:23.927664Z",
     "shell.execute_reply.started": "2025-03-23T04:34:23.913897Z"
    }
   },
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.backbone = nn.Sequential(\n",
    "        #   nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, ...)\n",
    "            nn.Conv2d(4, 32, 8, 4),      # (m, 4, 84, 84) -> (m, 32, 20, 20)\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(32, 64, 4, 2),     # (m, 32, 20, 20) -> (m, 64, 9, 9)\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(64, 64, 3, 1),     # (m, 64, 9, 9) -> (m, 64, 7, 7)\n",
    "            nn.SiLU(),\n",
    "            nn.Flatten(),                # (m, 64, 7, 7) -> (m, 3136)\n",
    "            nn.Linear(3136, 512),        # (m, 3136) -> (m, 512)\n",
    "            nn.SiLU()\n",
    "        )        \n",
    "        self.policy_head = nn.Linear(512, 4)  # -> output (m, 4)\n",
    "        self.value_head = nn.Linear(512, 1)   # -> output (m, 1)\n",
    "\n",
    "    @staticmethod\n",
    "    def preprocess(x):\n",
    "        x = x.float()\n",
    "        if len(x.size()) == 3:\n",
    "            x = x.unsqueeze(0)  # add the batch dim\n",
    "        with torch.no_grad():\n",
    "            x = x / 255.0  # should we scale to [0, 1]\n",
    "        x.requires_grad_()\n",
    "        return x\n",
    "\n",
    "    def value_only(self, x):\n",
    "        x = self.preprocess(x)\n",
    "        x = self.backbone(x)\n",
    "        value = self.value_head(x)\n",
    "        return value\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.preprocess(x)\n",
    "        x = self.backbone(x)\n",
    "        logits = self.policy_head(x)\n",
    "        value = self.value_head(x)\n",
    "        \n",
    "        return logits, value\n",
    "\n",
    "    def sample(self, state, stochastic=True):\n",
    "        logits, value = self.forward(state)\n",
    "        dist = Categorical(logits=logits)\n",
    "        if stochastic:\n",
    "            action = dist.sample()\n",
    "        else:\n",
    "            action = torch.argmax(logits, dim=-1)\n",
    "        log_prob = dist.log_prob(action)\n",
    "        \n",
    "        return action, log_prob, value\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T04:34:23.929766Z",
     "iopub.status.busy": "2025-03-23T04:34:23.929481Z",
     "iopub.status.idle": "2025-03-23T04:34:23.947458Z",
     "shell.execute_reply": "2025-03-23T04:34:23.946781Z",
     "shell.execute_reply.started": "2025-03-23T04:34:23.929735Z"
    }
   },
   "outputs": [],
   "source": [
    "def scope():  # check env is set up correctly\n",
    "    e = make_env()\n",
    "    e.reset()\n",
    "    for i in range(500):\n",
    "        if i % 10 == 0:\n",
    "            clear_output(wait=True)\n",
    "        s, r, term, trunc, _ = e.step(e.action_space.sample())\n",
    "        print('iter', i, (r,term,trunc))\n",
    "        if term or trunc:\n",
    "            sout = ''\n",
    "            if term: sout += 'terminated'\n",
    "            if trunc: sout += 'truncated'  # truncates at max_episode_steps\n",
    "            print(sout)\n",
    "    e.close()\n",
    "# scope()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T04:34:23.949719Z",
     "iopub.status.busy": "2025-03-23T04:34:23.949533Z",
     "iopub.status.idle": "2025-03-23T04:34:23.968967Z",
     "shell.execute_reply": "2025-03-23T04:34:23.968192Z",
     "shell.execute_reply.started": "2025-03-23T04:34:23.949702Z"
    }
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def collect_rollout(envs, network: ActorCritic, min_steps):\n",
    "    min_steps /= envs.num_envs\n",
    "    state = torch.as_tensor(envs.reset()[0], dtype=torch.uint8).to(device)\n",
    "    states, actions, rewards, terminateds, truncateds, log_probs, values = [], [], [], [], [], [], []\n",
    "    step = 0\n",
    "    while True:\n",
    "        step += 1\n",
    "        action, log_prob, value = network.sample(state)\n",
    "        next_state, reward, terminated, truncated, _ = envs.step(action)\n",
    "        next_state = torch.as_tensor(next_state, dtype=torch.uint8).to(device)\n",
    "        reward = torch.as_tensor(reward, dtype=torch.float)\n",
    "        terminated = torch.as_tensor(terminated, dtype=torch.float)\n",
    "        truncated = torch.as_tensor(truncated, dtype=torch.float)\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        rewards.append(reward)\n",
    "        terminateds.append(terminated)\n",
    "        truncateds.append(truncated)\n",
    "        log_probs.append(log_prob)\n",
    "        values.append(value)\n",
    "        state = next_state\n",
    "        if step >= min_steps and (terminated.any() or truncated.any()):\n",
    "            break\n",
    "    values.append(network.value_only(state))  # add the last value\n",
    "    \n",
    "    states = torch.stack(states, 1)                             # [n_envs, T, 4,84,84]\n",
    "    actions = torch.stack(actions, 1)                           # [n_envs, T]\n",
    "    rewards = torch.stack(rewards, 1).to(device)\n",
    "    terminateds = torch.stack(terminateds, 1).to(device)\n",
    "    truncateds = torch.stack(truncateds, 1).to(device)\n",
    "    values = torch.stack(values, 1).squeeze()                   # [n_envs, T+1]\n",
    "    log_probs = torch.stack(log_probs, 1)\n",
    "    truncateds[:,-1] = 1  # set truncation flag\n",
    "    \n",
    "    return states, actions, rewards, terminateds, truncateds, log_probs, values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T04:34:23.970722Z",
     "iopub.status.busy": "2025-03-23T04:34:23.970433Z",
     "iopub.status.idle": "2025-03-23T04:34:23.993545Z",
     "shell.execute_reply": "2025-03-23T04:34:23.992764Z",
     "shell.execute_reply.started": "2025-03-23T04:34:23.970689Z"
    }
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(network: ActorCritic, stochastic=False, render_mode=None, graph=False):\n",
    "    _env = make_env(render_mode)\n",
    "    state = torch.as_tensor(_env.reset()[0], dtype=torch.uint8).to(device)\n",
    "    sum_rewards = 0\n",
    "    if graph:\n",
    "        values = []\n",
    "        immediate_reward = []\n",
    "        rewards_collected = []\n",
    "        values_plus_rewards = []\n",
    "    step = 0\n",
    "    while True:\n",
    "        step += 1\n",
    "        action, log_prob, value = network.sample(state, stochastic=stochastic)\n",
    "        next_state, reward, terminated, truncated, _ = _env.step(action)\n",
    "        sum_rewards += reward\n",
    "        if graph:\n",
    "            values += [value.item()]\n",
    "            immediate_reward += [reward]\n",
    "            rewards_collected += [sum_rewards]\n",
    "            values_plus_rewards += [value.item() + sum_rewards - reward]\n",
    "            if step % 20 == 0 or terminated or truncated:\n",
    "                plt.plot(values, label='values')\n",
    "                plt.plot(immediate_reward, label='immediate reward')\n",
    "                plt.plot(rewards_collected, label='rewards collected')\n",
    "                plt.plot(values_plus_rewards, label='rewards + values')\n",
    "                plt.legend()\n",
    "                clear_output(wait=True)\n",
    "                plt.show()\n",
    "        state = torch.as_tensor(next_state, dtype=torch.uint8).to(device)\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "    _env.close()\n",
    "    return sum_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T04:34:23.994562Z",
     "iopub.status.busy": "2025-03-23T04:34:23.994342Z",
     "iopub.status.idle": "2025-03-23T04:34:24.014810Z",
     "shell.execute_reply": "2025-03-23T04:34:24.014285Z",
     "shell.execute_reply.started": "2025-03-23T04:34:23.994544Z"
    }
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def gae(rewards, terminateds, truncateds, values, gamma, lambda_):\n",
    "    assert not (truncateds[:,:-1].any())  # for now, assume only truncation is the last step\n",
    "    td_errors = rewards + gamma * values[:,1:] * (1 - terminateds) - values[:,:-1]  # on truncation bootstrap from the next value\n",
    "    advantages = torch.zeros_like(td_errors)\n",
    "    advantage = torch.zeros_like(td_errors[:,0])\n",
    "    for t in reversed(range(td_errors.size(1))):\n",
    "        advantage = lambda_ * gamma * advantage * (1 - terminateds[:,t]) + td_errors[:,t]\n",
    "        advantages[:,t] = advantage\n",
    "    return advantages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T04:34:24.015692Z",
     "iopub.status.busy": "2025-03-23T04:34:24.015488Z",
     "iopub.status.idle": "2025-03-23T04:34:24.031593Z",
     "shell.execute_reply": "2025-03-23T04:34:24.031011Z",
     "shell.execute_reply.started": "2025-03-23T04:34:24.015674Z"
    }
   },
   "outputs": [],
   "source": [
    "def explained_variance(returns, values):  # measure of critic's accuracy: 0 = random guessing, 1 = perfect\n",
    "    var_returns = torch.var(returns)\n",
    "    var_residuals = torch.var(returns - values)\n",
    "    if var_returns == 0: return 0.0 if var_residuals == 0 else -float('inf')\n",
    "    return (1 - var_residuals / var_returns).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T04:34:24.032431Z",
     "iopub.status.busy": "2025-03-23T04:34:24.032257Z",
     "iopub.status.idle": "2025-03-23T04:34:24.054415Z",
     "shell.execute_reply": "2025-03-23T04:34:24.053653Z",
     "shell.execute_reply.started": "2025-03-23T04:34:24.032415Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "num_steps = 8192  # steps per rollout\n",
    "num_epochs = 10   # epochs per iteration\n",
    "gae_lambda = 0.95      # GAE(lambda): 0 is same as TD-error, 1 is same as Monte Carlo return\n",
    "value_weight = 0.5     # weight of value loss (vs policy loss)\n",
    "entropy_bonus = 0.01 * 1.0  # entropy regularization coefficient in loss, may need to manually adjust with reward scale\n",
    "clip_eps = 0.2         # policy ratio clipping radius\n",
    "clip_eps_value = None   # value clipping radius\n",
    "gamma = 0.99\n",
    "policy_lr = 3e-4\n",
    "value_lr = 3e-4\n",
    "backbone_lr = 3e-4  # lr for the shared backbone, OpenAI's Spinning Up implicity uses same lr as critic\n",
    "max_norm = 0.5\n",
    "\n",
    "checkpoint_freq = 40\n",
    "eval_freq = 20\n",
    "\n",
    "lr_decay = .999\n",
    "entropy_decay = .999\n",
    "\n",
    "# policy_lr *= 100\n",
    "# value_lr *= 100\n",
    "# backbone_lr *= 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T04:36:31.705962Z",
     "iopub.status.busy": "2025-03-23T04:36:31.705610Z",
     "iopub.status.idle": "2025-03-23T04:36:39.219752Z",
     "shell.execute_reply": "2025-03-23T04:36:39.218998Z",
     "shell.execute_reply.started": "2025-03-23T04:36:31.705914Z"
    }
   },
   "outputs": [],
   "source": [
    "# try: envs.reset(), envs.close()\n",
    "# except: pass\n",
    "# envs = make_envs()\n",
    "\n",
    "network = ActorCritic().to(device)\n",
    "\n",
    "# if compiling\n",
    "# import cl_fix\n",
    "# if KAGGLE:\n",
    "    # network = torch.compile(network)\n",
    "\n",
    "shared_backbone_params = list(network.backbone.parameters())\n",
    "policy_head_params = list(network.policy_head.parameters())\n",
    "value_head_params = list(network.value_head.parameters())\n",
    "\n",
    "optimizer = torch.optim.Adam([  # single optimizer, avoids ambiguity in lr of backbone that is present in Spinning Up\n",
    "    {'params': policy_head_params, 'lr': policy_lr},\n",
    "    {'params': value_head_params, 'lr': value_lr},\n",
    "    {'params': shared_backbone_params, 'lr': backbone_lr},\n",
    "])\n",
    "\n",
    "wandb.finish()\n",
    "wandb.init(\n",
    "    project='PPO-' + env_name.replace('/', '_') + ' (discrete)',\n",
    "    name='no norm vec' + (' (Kaggle)' if KAGGLE else ''),\n",
    "    # mode='disabled',\n",
    "    config={\n",
    "        'batch_size': batch_size,\n",
    "        'num_steps': num_steps,\n",
    "        'num_epochs': num_epochs,\n",
    "        'max_norm': max_norm,\n",
    "        'gae_lambda': gae_lambda,\n",
    "        'value_weight': value_weight,\n",
    "        'entropy_bonus': entropy_bonus,\n",
    "        'clip_eps': clip_eps,\n",
    "        'clip_eps_value': clip_eps_value,\n",
    "        'gamma': gamma,\n",
    "        'policy_lr': policy_lr,\n",
    "        'value_lr': value_lr,\n",
    "        'backbone_lr': backbone_lr,\n",
    "        'env_name': env_name,\n",
    "        'device': device,\n",
    "    }\n",
    ")\n",
    "\n",
    "history = {'step_count': 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T04:36:39.221071Z",
     "iopub.status.busy": "2025-03-23T04:36:39.220835Z",
     "iopub.status.idle": "2025-03-23T04:36:39.225608Z",
     "shell.execute_reply": "2025-03-23T04:36:39.224993Z",
     "shell.execute_reply.started": "2025-03-23T04:36:39.221050Z"
    }
   },
   "outputs": [],
   "source": [
    "def save(network, history):\n",
    "    os.makedirs('saved', exist_ok=True)\n",
    "    data = {\n",
    "        'network_state_dict': network.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'history': history\n",
    "    }\n",
    "    name = './saved/breakout-' + str(history['step_count']) + 's.pth'\n",
    "    torch.save(data, name)\n",
    "    print('saved to ' + name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T04:36:39.227105Z",
     "iopub.status.busy": "2025-03-23T04:36:39.226801Z",
     "iopub.status.idle": "2025-03-23T04:36:39.253923Z",
     "shell.execute_reply": "2025-03-23T04:36:39.253345Z",
     "shell.execute_reply.started": "2025-03-23T04:36:39.227077Z"
    }
   },
   "outputs": [],
   "source": [
    "def load(network, optimizer, history, path):\n",
    "    data = torch.load(path, weights_only=False, map_location=torch.device(device))\n",
    "    network.load_state_dict(data['network_state_dict'])\n",
    "    optimizer.load_state_dict(data['optimizer_state_dict'])\n",
    "    history.update(data['history'])\n",
    "    print('loaded from ' + path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T04:39:12.853549Z",
     "iopub.status.busy": "2025-03-23T04:39:12.853214Z",
     "iopub.status.idle": "2025-03-23T04:39:12.871343Z",
     "shell.execute_reply": "2025-03-23T04:39:12.870690Z",
     "shell.execute_reply.started": "2025-03-23T04:39:12.853513Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(envs, num_updates, history):\n",
    "    global entropy_bonus\n",
    "    \n",
    "    t_start = time.time()\n",
    "    try:\n",
    "        for update in range(1, num_updates+1):\n",
    "            if update % checkpoint_freq == 0:\n",
    "                save(network, history)\n",
    "            \n",
    "            t0 = time.time()\n",
    "            states, actions, rewards, terminateds, truncateds, log_probs, values = collect_rollout(envs, network, num_steps) \n",
    "            rollout_time = time.time() - t0\n",
    "            \n",
    "            t0 = time.time()\n",
    "            gae_advantages = gae(rewards, terminateds, truncateds, values, gamma, gae_lambda)\n",
    "            values = values[:,:-1]  # remove the last values after gae\n",
    "            advantages = gae_advantages\n",
    "            # advantages = (gae_advantages - gae_advantages.mean()) / (gae_advantages.std() + 1e-7)  # careful with standardization\n",
    "            \n",
    "            advantages = advantages.ravel()\n",
    "            values = values.ravel()\n",
    "            log_probs = log_probs.ravel()\n",
    "            actions = actions.ravel()\n",
    "            states = states.reshape(-1, *states.shape[2:])\n",
    "            assert advantages.size(0) == values.size(0) == log_probs.size(0) == actions.size(0) == states.size(0)\n",
    "            \n",
    "            returns = advantages + values\n",
    "            actual_steps = states.size(0)            \n",
    "            \n",
    "            # stats across all minibatches\n",
    "            mb_policy_losses = []\n",
    "            mb_value_losses = []\n",
    "            mb_entropies = []\n",
    "            mb_kls = []\n",
    "            mb_clip_fractions = []\n",
    "            mb_global_l2_norms = []\n",
    "            \n",
    "            for _ in range(num_epochs):\n",
    "                indices = torch.randperm(actual_steps)\n",
    "                for begin in range(0, actual_steps, batch_size):\n",
    "                    end = begin + batch_size\n",
    "                    batch_indices = indices[begin:end]\n",
    "                    \n",
    "                    states_b = states[batch_indices]\n",
    "                    actions_b = actions[batch_indices]\n",
    "                    returns_b = returns[batch_indices]\n",
    "                    advantages_b = advantages[batch_indices]\n",
    "                    old_log_probs = log_probs[batch_indices]\n",
    "                    \n",
    "                    logits, values_b = network(states_b)\n",
    "                    values_b = values_b.squeeze()\n",
    "                    dist = Categorical(logits=logits)\n",
    "                    entropy = dist.entropy().mean()\n",
    "                    cur_log_probs = dist.log_prob(actions_b.squeeze()).reshape(old_log_probs.shape)\n",
    "                \n",
    "                    # policy loss\n",
    "                    ratio = torch.exp(cur_log_probs - old_log_probs)  # same as cur_probs/old_probs, just with logs\n",
    "                    fst_surrogate = ratio * advantages_b\n",
    "                    snd_surrogate = torch.clip(ratio, 1 - clip_eps, 1 + clip_eps) * advantages_b\n",
    "                    policy_loss = -torch.min(fst_surrogate, snd_surrogate).mean() - entropy_bonus * entropy\n",
    "                    \n",
    "                    # value loss\n",
    "                    if clip_eps_value is not None:\n",
    "                        old_values_b = values[batch_indices]\n",
    "                        values_clipped = old_values_b + (values_b - old_values_b).clip(-clip_eps_value, clip_eps_value)\n",
    "                        fst_loss = F.huber_loss(values_clipped, returns_b, reduction='none')\n",
    "                        snd_loss = F.huber_loss(values_b, returns_b, reduction='none')\n",
    "                        value_loss = value_weight * torch.max(fst_loss, snd_loss).mean()  # take the more conservative one\n",
    "                    else:\n",
    "                        value_loss = value_weight * F.huber_loss(values_b, returns_b, reduction='mean')\n",
    "    \n",
    "                    cum_loss = policy_loss + value_loss\n",
    "                    optimizer.zero_grad()\n",
    "                    cum_loss.backward()\n",
    "                    global_l2_norm = nn.utils.clip_grad_norm_(network.parameters(), max_norm)\n",
    "                    optimizer.step()\n",
    "                    \n",
    "                    # Track statistics for this minibatch\n",
    "                    mb_policy_losses.append(policy_loss.item())\n",
    "                    mb_value_losses.append(value_loss.item())\n",
    "                    mb_entropies.append(entropy.item())\n",
    "                    mb_kls.append((old_log_probs - cur_log_probs).mean().item())\n",
    "                    clip_fraction = ((ratio > 1 + clip_eps) | (ratio < 1 - clip_eps)).float().mean().item()\n",
    "                    mb_clip_fractions.append(clip_fraction)\n",
    "                    mb_global_l2_norms.append(global_l2_norm)\n",
    "                    \n",
    "            # average stats across minibatches\n",
    "            avg_policy_loss = sum(mb_policy_losses) / len(mb_policy_losses)\n",
    "            avg_value_loss = sum(mb_value_losses) / len(mb_value_losses)\n",
    "            avg_entropy = sum(mb_entropies) / len(mb_entropies)\n",
    "            avg_kl = sum(mb_kls) / len(mb_kls)\n",
    "            avg_clip_fraction = sum(mb_clip_fractions) / len(mb_clip_fractions)\n",
    "            avg_global_l2_norm = sum(mb_global_l2_norms) / len(mb_global_l2_norms)\n",
    "            \n",
    "            history['step_count'] += actual_steps        \n",
    "            \n",
    "            update_time = time.time() - t0\n",
    "            wall_time = time.time() - t_start\n",
    "    \n",
    "            t0 = time.time()\n",
    "            with torch.no_grad():\n",
    "                exp_var = explained_variance(returns, values)\n",
    "            stats_time = time.time() - t0\n",
    "                \n",
    "            log = {\n",
    "                'entropy': avg_entropy,\n",
    "                'explained_variance': exp_var,\n",
    "                'loss_policy': avg_policy_loss,  # entropy bonus incl.\n",
    "                'loss_value': avg_value_loss,\n",
    "                'loss_entropy': -entropy_bonus * avg_entropy,\n",
    "                'loss_cum': avg_policy_loss + avg_value_loss,\n",
    "                'approx_kl': avg_kl,  # average across all minibatches\n",
    "                'clip_fraction': avg_clip_fraction,  # (policy ratio)\n",
    "                'global_l2_norm': avg_global_l2_norm,\n",
    "                'advantage': gae_advantages.mean().item(),  # pre standardization\n",
    "                'update': update,\n",
    "                'step_count': history['step_count'],\n",
    "                'approx_fps': actual_steps / (update_time + rollout_time),\n",
    "                'time_wall': wall_time,\n",
    "                'time_update': update_time,\n",
    "                'time_rollout': rollout_time,\n",
    "                'time_stats': stats_time,\n",
    "                'policy_lr': optimizer.param_groups[0]['lr'],\n",
    "                'entropy_bonus': entropy_bonus,\n",
    "            }\n",
    "    \n",
    "            if update % eval_freq == 0 or update == num_updates:\n",
    "                t0 = time.time()\n",
    "                eval_rewards_d = evaluate(network, stochastic=False)\n",
    "                eval_rewards_s = evaluate(network, stochastic=True)\n",
    "                eval_time = time.time() - t0\n",
    "                log.update({\n",
    "                    'eval_time': eval_time,\n",
    "                    'eval_rewards (deterministic)': eval_rewards_d,\n",
    "                    'eval_rewards (stochastic)': eval_rewards_s,\n",
    "                })\n",
    "    \n",
    "                clear_output(wait=True)\n",
    "                print(f'wall time {wall_time:.2f} s')\n",
    "                print(f'update {update}')\n",
    "                print(f'step {history[\"step_count\"]}')\n",
    "                print(f'eval rewards (deterministic) {eval_rewards_d:.5f}')\n",
    "                print(f'eval rewards (stochastic) {eval_rewards_s:.5f}')\n",
    "    \n",
    "            try: wandb.log(log, step=history['step_count'])\n",
    "            except: print(f'[warning] wandb log failed')\n",
    "                \n",
    "            for group in optimizer.param_groups:\n",
    "                group['lr'] *= lr_decay\n",
    "            entropy_bonus *= entropy_decay\n",
    "\n",
    "            if wall_time > 3600*30:\n",
    "                return\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        save(network, history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T04:36:39.276546Z",
     "iopub.status.busy": "2025-03-23T04:36:39.276348Z",
     "iopub.status.idle": "2025-03-23T04:36:39.300040Z",
     "shell.execute_reply": "2025-03-23T04:36:39.299154Z",
     "shell.execute_reply.started": "2025-03-23T04:36:39.276523Z"
    }
   },
   "outputs": [],
   "source": [
    "load(network, optimizer, history, '/kaggle/input/vecced2/breakout-15297792s-vec.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T04:39:16.668442Z",
     "iopub.status.busy": "2025-03-23T04:39:16.668090Z",
     "iopub.status.idle": "2025-03-23T04:40:29.982024Z",
     "shell.execute_reply": "2025-03-23T04:40:29.970148Z",
     "shell.execute_reply.started": "2025-03-23T04:39:16.668412Z"
    }
   },
   "outputs": [],
   "source": [
    "if KAGGLE:\n",
    "    import multiprocessing\n",
    "    multiprocessing.set_start_method('spawn', force=True)\n",
    "\n",
    "try: envs.reset(), envs.close()\n",
    "except: pass\n",
    "envs = make_envs(32)\n",
    "\n",
    "t0 = time.time()\n",
    "try:\n",
    "    train(envs, 999999, history)\n",
    "except KeyboardInterrupt:\n",
    "    print('keyboard interrupt. stopping...')\n",
    "finally:\n",
    "    print(f'done.\\n*actual* time taken: {time.time() - t0:.2f} s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-03-23T04:35:24.218207Z",
     "iopub.status.idle": "2025-03-23T04:35:24.218492Z",
     "shell.execute_reply": "2025-03-23T04:35:24.218378Z"
    }
   },
   "outputs": [],
   "source": [
    "if not KAGGLE:\n",
    "    evaluate(network, stochastic=True, render_mode='human', graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6951015,
     "sourceId": 11143089,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6952108,
     "sourceId": 11144531,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
