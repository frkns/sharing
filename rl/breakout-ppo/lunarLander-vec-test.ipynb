{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43e70baa-0709-481c-a4af-53e2c31a22a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "KAGGLE = 'KAGGLE_KERNEL_RUN_TYPE' in os.environ  # detect if notebook is running on Kaggle\n",
    "if KAGGLE:\n",
    "    !pip install gymnasium[atari]==1.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7993116-5dcb-42f7-8fe6-970989dc7771",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium.wrappers import Autoreset, TimeAwareObservation, FrameStackObservation, AtariPreprocessing, ClipReward\n",
    "from collections import defaultdict, deque\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import random\n",
    "import ale_py\n",
    "import wandb\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "daf418e9-fde6-40ad-9b6c-7e366f03db1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1.0\n",
      "2.6.0+cpu\n",
      "10\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "print(gym.__version__, torch.__version__, torch.get_num_threads(), torch.get_num_interop_threads(), sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9ab5096-bac4-4cb4-b5ba-4d6802e66ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# globals\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "env_name = 'LunarLander-v3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "025715db-68f9-4266-8ba8-a03c67b7069d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(render_mode=None):\n",
    "    env = gym.make(env_name, render_mode=render_mode, max_episode_steps=500)\n",
    "    # env = TimeAwareObservation(env)\n",
    "    env = ClipReward(env, -1, 1)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3690f853-676f-4caf-952d-aa3dde55621f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_envs(n_envs=32):  # >32 might cause memory issues\n",
    "    envs = gym.make_vec(\n",
    "        env_name, \n",
    "        max_episode_steps=500,\n",
    "        num_envs=n_envs, \n",
    "        vectorization_mode='async', \n",
    "        vector_kwargs={\n",
    "            # 'autoreset_mode': gym.vector.AutoresetMode.DISABLED,\n",
    "        },\n",
    "        wrappers=[  # make_vec autoresets by default\n",
    "            lambda env: ClipReward(env, -1, 1),\n",
    "        ]\n",
    "    )\n",
    "    return envs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "170e4f10-e7b2-49e9-a398-c0d5891d4511",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.backbone = nn.Sequential(\n",
    "            nn.Linear(8, 64), \n",
    "            nn.SiLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.SiLU()\n",
    "        )        \n",
    "        self.policy_head = nn.Linear(64, 4)  # -> output (m, 4)\n",
    "        self.value_head = nn.Linear(64, 1)   # -> output (m, 1)\n",
    "\n",
    "    @staticmethod\n",
    "    def preprocess(x):\n",
    "        if len(x.size()) == 1:\n",
    "            x = x.unsqueeze(0)  # add the batch dim\n",
    "        with torch.no_grad():\n",
    "            # x = x / 255.0  # should we scale to [0, 1]\n",
    "            pass\n",
    "        x.requires_grad_()\n",
    "        return x\n",
    "\n",
    "    def value_only(self, x):\n",
    "        x = self.preprocess(x)\n",
    "        x = self.backbone(x)\n",
    "        value = self.value_head(x)\n",
    "        return value\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.preprocess(x)\n",
    "        x = self.backbone(x)\n",
    "        logits = self.policy_head(x)\n",
    "        value = self.value_head(x)\n",
    "        \n",
    "        return logits, value\n",
    "\n",
    "    def sample(self, state, stochastic=True):\n",
    "        logits, value = self.forward(state)\n",
    "        dist = Categorical(logits=logits)\n",
    "        if stochastic:\n",
    "            action = dist.sample()\n",
    "        else:\n",
    "            action = torch.argmax(logits, dim=-1)\n",
    "        log_prob = dist.log_prob(action)\n",
    "        \n",
    "        return action, log_prob, value\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d15ed1a-f99a-48f4-8c3f-f15c85fe55d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scope():  # check env is set up correctly\n",
    "    e = make_env()\n",
    "    e.reset()\n",
    "    for i in range(500):\n",
    "        if i % 10 == 0:\n",
    "            clear_output(wait=True)\n",
    "        s, r, term, trunc, _ = e.step(e.action_space.sample())\n",
    "        print('iter', i, (r,term,trunc))\n",
    "        if term or trunc:\n",
    "            sout = ''\n",
    "            if term: sout += 'terminated'\n",
    "            if trunc: sout += 'truncated'  # truncates at max_episode_steps\n",
    "            print(sout)\n",
    "    e.close()\n",
    "# scope()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8930ebf8-5b27-49aa-87a0-90e773273bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def collect_rollout(envs, network: ActorCritic, min_steps):\n",
    "    min_steps /= envs.num_envs\n",
    "    state = torch.as_tensor(envs.reset()[0], dtype=torch.float).to(device)\n",
    "    states, actions, rewards, terminateds, truncateds, log_probs, values = [], [], [], [], [], [], []\n",
    "    step = 0\n",
    "    while True:\n",
    "        step += 1\n",
    "        action, log_prob, value = network.sample(state)\n",
    "        next_state, reward, terminated, truncated, _ = envs.step(action.numpy().squeeze())\n",
    "        next_state = torch.as_tensor(next_state, dtype=torch.float).to(device)\n",
    "        reward = torch.as_tensor(reward, dtype=torch.float)\n",
    "        terminated = torch.as_tensor(terminated, dtype=torch.float)\n",
    "        truncated = torch.as_tensor(truncated, dtype=torch.float)\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        rewards.append(reward)\n",
    "        terminateds.append(terminated)\n",
    "        truncateds.append(truncated)\n",
    "        log_probs.append(log_prob)\n",
    "        values.append(value)\n",
    "        state = next_state\n",
    "        if step >= min_steps and (terminated.any() or truncated.any()):\n",
    "            break\n",
    "    values.append(network.value_only(state))  # add the last value\n",
    "    \n",
    "    states = torch.stack(states, 1)                             # [n_envs, T, 4,84,84]\n",
    "    actions = torch.stack(actions, 1)                           # [n_envs, T]\n",
    "    rewards = torch.stack(rewards, 1).to(device)\n",
    "    terminateds = torch.stack(terminateds, 1).to(device)\n",
    "    truncateds = torch.stack(truncateds, 1).to(device)\n",
    "    values = torch.stack(values, 1).squeeze()                   # [n_envs, T+1]\n",
    "    log_probs = torch.stack(log_probs, 1)\n",
    "    truncateds[:,-1] = 1  # set truncation flag\n",
    "    \n",
    "    return states, actions, rewards, terminateds, truncateds, log_probs, values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d54aa200-3589-458c-bb59-6113e406ba9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(network: ActorCritic, stochastic=False, render_mode=None, graph=False):\n",
    "    _env = make_env(render_mode)\n",
    "    state = torch.as_tensor(_env.reset()[0], dtype=torch.float).to(device)\n",
    "    sum_rewards = 0\n",
    "    if graph:\n",
    "        values = []\n",
    "        immediate_reward = []\n",
    "        rewards_collected = []\n",
    "        values_plus_rewards = []\n",
    "    step = 0\n",
    "    while True:\n",
    "        step += 1\n",
    "        action, log_prob, value = network.sample(state, stochastic=stochastic)\n",
    "        next_state, reward, terminated, truncated, _ = _env.step(action.numpy().squeeze())\n",
    "        sum_rewards += reward\n",
    "        if graph:\n",
    "            values += [value.item()]\n",
    "            immediate_reward += [reward]\n",
    "            rewards_collected += [sum_rewards]\n",
    "            values_plus_rewards += [value.item() + sum_rewards - reward]\n",
    "            if step % 20 == 0 or terminated or truncated:\n",
    "                plt.plot(values, label='values')\n",
    "                plt.plot(immediate_reward, label='immediate reward')\n",
    "                plt.plot(rewards_collected, label='rewards collected')\n",
    "                plt.plot(values_plus_rewards, label='rewards + values')\n",
    "                plt.legend()\n",
    "                clear_output(wait=True)\n",
    "                plt.show()\n",
    "        state = torch.as_tensor(next_state, dtype=torch.float).to(device)\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "    _env.close()\n",
    "    return sum_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d2a5fbb4-1b57-4a2b-8c12-5aab6accdeab",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def gae(rewards, terminateds, truncateds, values, gamma, lambda_):\n",
    "    # assert not (truncateds[:,:-1].any())  # for now, assume only truncation is the last step\n",
    "    td_errors = rewards + gamma * values[:,1:] * (1 - terminateds) - values[:,:-1]  # on truncation bootstrap from the next value\n",
    "    advantages = torch.zeros_like(td_errors)\n",
    "    advantage = torch.zeros_like(td_errors[:,0])\n",
    "    for t in reversed(range(td_errors.size(1))):\n",
    "        advantage = lambda_ * gamma * advantage * (1 - terminateds[:,t]) + td_errors[:,t]\n",
    "        advantages[:,t] = advantage\n",
    "    return advantages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a5f8a065-cc28-4536-8e7a-04b48e2321e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explained_variance(returns, values):  # measure of critic's accuracy: 0 = random guessing, 1 = perfect\n",
    "    var_returns = torch.var(returns)\n",
    "    var_residuals = torch.var(returns - values)\n",
    "    if var_returns == 0: return 0.0 if var_residuals == 0 else -float('inf')\n",
    "    return (1 - var_residuals / var_returns).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f01436fc-7f3f-468a-8c71-f7619df58f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "num_steps = 4096*4 # steps per rollout\n",
    "num_epochs = 10   # epochs per iteration\n",
    "gae_lambda = 0.95      # GAE(lambda): 0 is same as TD-error, 1 is same as Monte Carlo return\n",
    "value_weight = 0.5     # weight of value loss (vs policy loss)\n",
    "entropy_bonus = 0.01 * 1.0  # entropy regularization coefficient in loss, may need to manually adjust with reward scale\n",
    "clip_eps = 0.2         # policy ratio clipping radius\n",
    "clip_eps_value = None   # value clipping radius\n",
    "gamma = 0.99\n",
    "policy_lr = 3e-4\n",
    "value_lr = 3e-4\n",
    "backbone_lr = 3e-4  # lr for the shared backbone, OpenAI's Spinning Up implicity uses same lr as critic\n",
    "max_norm = 0.5\n",
    "\n",
    "checkpoint_freq = 40\n",
    "eval_freq = 20\n",
    "\n",
    "lr_decay = .9998\n",
    "entropy_decay = .9998\n",
    "\n",
    "# policy_lr *= 100\n",
    "# value_lr *= 100\n",
    "# backbone_lr *= 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c055d6db-7f80-4943-85a2-f9b2b6a5427e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>advantage</td><td>▁▃▄▅▇▇█</td></tr><tr><td>approx_fps</td><td>▇█▂▁▂▂▂</td></tr><tr><td>approx_kl</td><td>▂▄█▄▁▂▁</td></tr><tr><td>clip_fraction</td><td>▂▅█▅▄▂▁</td></tr><tr><td>entropy</td><td>█▇▆▄▂▁▁</td></tr><tr><td>entropy_bonus</td><td>█▇▆▄▃▂▁</td></tr><tr><td>explained_variance</td><td>▁▃▄▅▇▇█</td></tr><tr><td>global_l2_norm</td><td>▁▂▂▃▄▆█</td></tr><tr><td>loss_cum</td><td>▇██▇▄▂▁</td></tr><tr><td>loss_entropy</td><td>▁▂▃▅▇██</td></tr><tr><td>loss_policy</td><td>▃▁▂▅▆██</td></tr><tr><td>loss_value</td><td>▇██▇▄▁▁</td></tr><tr><td>policy_lr</td><td>█▇▆▄▃▂▁</td></tr><tr><td>step_count</td><td>▁▂▃▄▆▇█</td></tr><tr><td>time_rollout</td><td>▁▁▇██▆▇</td></tr><tr><td>time_stats</td><td>▅▅▅▅▅█▁</td></tr><tr><td>time_update</td><td>▂▁▅▇▆▄█</td></tr><tr><td>time_wall</td><td>▁▂▃▄▆▇█</td></tr><tr><td>update</td><td>▁▂▃▅▆▇█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>advantage</td><td>-0.73833</td></tr><tr><td>approx_fps</td><td>1490.07266</td></tr><tr><td>approx_kl</td><td>0.00681</td></tr><tr><td>clip_fraction</td><td>0.06386</td></tr><tr><td>entropy</td><td>1.11145</td></tr><tr><td>entropy_bonus</td><td>0.00999</td></tr><tr><td>explained_variance</td><td>0.78441</td></tr><tr><td>global_l2_norm</td><td>0.21713</td></tr><tr><td>loss_cum</td><td>0.13629</td></tr><tr><td>loss_entropy</td><td>-0.0111</td></tr><tr><td>loss_policy</td><td>-0.01714</td></tr><tr><td>loss_value</td><td>0.15343</td></tr><tr><td>policy_lr</td><td>0.0003</td></tr><tr><td>step_count</td><td>58976</td></tr><tr><td>time_rollout</td><td>3.41289</td></tr><tr><td>time_stats</td><td>0</td></tr><tr><td>time_update</td><td>2.34253</td></tr><tr><td>time_wall</td><td>37.79214</td></tr><tr><td>update</td><td>7</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">vec</strong> at: <a href='https://wandb.ai/kchau-university-of-calgary-in-alberta/PPO-LunarLander-v3%20%28discrete%29/runs/sya52rpl' target=\"_blank\">https://wandb.ai/kchau-university-of-calgary-in-alberta/PPO-LunarLander-v3%20%28discrete%29/runs/sya52rpl</a><br> View project at: <a href='https://wandb.ai/kchau-university-of-calgary-in-alberta/PPO-LunarLander-v3%20%28discrete%29' target=\"_blank\">https://wandb.ai/kchau-university-of-calgary-in-alberta/PPO-LunarLander-v3%20%28discrete%29</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250322_233102-sya52rpl\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\flame\\wlink\\rl\\breakout-ppo\\wandb\\run-20250322_233256-tyzripob</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kchau-university-of-calgary-in-alberta/PPO-LunarLander-v3%20%28discrete%29/runs/tyzripob' target=\"_blank\">vec</a></strong> to <a href='https://wandb.ai/kchau-university-of-calgary-in-alberta/PPO-LunarLander-v3%20%28discrete%29' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kchau-university-of-calgary-in-alberta/PPO-LunarLander-v3%20%28discrete%29' target=\"_blank\">https://wandb.ai/kchau-university-of-calgary-in-alberta/PPO-LunarLander-v3%20%28discrete%29</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kchau-university-of-calgary-in-alberta/PPO-LunarLander-v3%20%28discrete%29/runs/tyzripob' target=\"_blank\">https://wandb.ai/kchau-university-of-calgary-in-alberta/PPO-LunarLander-v3%20%28discrete%29/runs/tyzripob</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# try: envs.reset(), envs.close()\n",
    "# except: pass\n",
    "# envs = make_envs()\n",
    "\n",
    "network = ActorCritic().to(device)\n",
    "\n",
    "# if compiling\n",
    "# import cl_fix\n",
    "# if KAGGLE:\n",
    "    # network = torch.compile(network)\n",
    "\n",
    "shared_backbone_params = list(network.backbone.parameters())\n",
    "policy_head_params = list(network.policy_head.parameters())\n",
    "value_head_params = list(network.value_head.parameters())\n",
    "\n",
    "optimizer = torch.optim.Adam([  # single optimizer, avoids ambiguity in lr of backbone that is present in Spinning Up\n",
    "    {'params': policy_head_params, 'lr': policy_lr},\n",
    "    {'params': value_head_params, 'lr': value_lr},\n",
    "    {'params': shared_backbone_params, 'lr': backbone_lr},\n",
    "])\n",
    "\n",
    "wandb.login(key='764fe2985db37b03500e798e1a8ea9ad359f81ec')\n",
    "wandb.finish()\n",
    "wandb.init(\n",
    "    project='PPO-' + env_name.replace('/', '_') + ' (discrete)',\n",
    "    name='vec' + (' (Kaggle)' if KAGGLE else ''),\n",
    "    # mode='disabled',\n",
    "    config={\n",
    "        'batch_size': batch_size,\n",
    "        'num_steps': num_steps,\n",
    "        'num_epochs': num_epochs,\n",
    "        'max_norm': max_norm,\n",
    "        'gae_lambda': gae_lambda,\n",
    "        'value_weight': value_weight,\n",
    "        'entropy_bonus': entropy_bonus,\n",
    "        'clip_eps': clip_eps,\n",
    "        'clip_eps_value': clip_eps_value,\n",
    "        'gamma': gamma,\n",
    "        'policy_lr': policy_lr,\n",
    "        'value_lr': value_lr,\n",
    "        'backbone_lr': backbone_lr,\n",
    "        'env_name': env_name,\n",
    "        'device': device,\n",
    "    }\n",
    ")\n",
    "\n",
    "history = {'step_count': 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a159391a-d4cd-4cbe-955d-ff05393c1790",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(network, history):\n",
    "    os.makedirs('saved', exist_ok=True)\n",
    "    # data = {\n",
    "    #     'network_state_dict': network.state_dict(),\n",
    "    #     'optimizer_state_dict': optimizer.state_dict(),\n",
    "    #     'history': history\n",
    "    # }\n",
    "    # name = './saved/breakout-' + str(history['step_count']) + 's.pth'\n",
    "    # torch.save(data, name)\n",
    "    # print('saved to ' + name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5c27dc7c-088a-44b5-aed5-7d657602a12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(network, optimizer, history, path):\n",
    "    data = torch.load(path, weights_only=False, map_location=torch.device(device))\n",
    "    network.load_state_dict(data['network_state_dict'])\n",
    "    optimizer.load_state_dict(data['optimizer_state_dict'])\n",
    "    history.update(data['history'])\n",
    "    print('loaded from ' + path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b99f1dbc-6cc3-4287-ba4e-6c38b1dd5951",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(envs, num_updates, history):\n",
    "    global entropy_bonus\n",
    "    \n",
    "    t_start = time.time()\n",
    "    try:\n",
    "        for update in range(1, num_updates+1):\n",
    "            if update % checkpoint_freq == 0:\n",
    "                save(network, history)\n",
    "            \n",
    "            t0 = time.time()\n",
    "            states, actions, rewards, terminateds, truncateds, log_probs, values = collect_rollout(envs, network, num_steps) \n",
    "            rollout_time = time.time() - t0\n",
    "            \n",
    "            t0 = time.time()\n",
    "            gae_advantages = gae(rewards, terminateds, truncateds, values, gamma, gae_lambda)\n",
    "            values = values[:,:-1]  # remove the last values after gae\n",
    "            advantages = (gae_advantages - gae_advantages.mean()) / (gae_advantages.std() + 1e-7)  # careful with standardization\n",
    "            \n",
    "            advantages = advantages.ravel()\n",
    "            values = values.ravel()\n",
    "            log_probs = log_probs.ravel()\n",
    "            actions = actions.ravel()\n",
    "            states = states.reshape(-1, *states.shape[2:])\n",
    "            assert advantages.size(0) == values.size(0) == log_probs.size(0) == actions.size(0) == states.size(0)\n",
    "            \n",
    "            returns = advantages + values\n",
    "            actual_steps = states.size(0)            \n",
    "            \n",
    "            # stats across all minibatches\n",
    "            mb_policy_losses = []\n",
    "            mb_value_losses = []\n",
    "            mb_entropies = []\n",
    "            mb_kls = []\n",
    "            mb_clip_fractions = []\n",
    "            mb_global_l2_norms = []\n",
    "            \n",
    "            for _ in range(num_epochs):\n",
    "                indices = torch.randperm(actual_steps)\n",
    "                for begin in range(0, actual_steps, batch_size):\n",
    "                    end = begin + batch_size\n",
    "                    batch_indices = indices[begin:end]\n",
    "                    \n",
    "                    states_b = states[batch_indices]\n",
    "                    actions_b = actions[batch_indices]\n",
    "                    returns_b = returns[batch_indices]\n",
    "                    advantages_b = advantages[batch_indices]\n",
    "                    old_log_probs = log_probs[batch_indices]\n",
    "                    \n",
    "                    logits, values_b = network(states_b)\n",
    "                    values_b = values_b.squeeze()\n",
    "                    dist = Categorical(logits=logits)\n",
    "                    entropy = dist.entropy().mean()\n",
    "                    cur_log_probs = dist.log_prob(actions_b.squeeze()).reshape(old_log_probs.shape)\n",
    "                \n",
    "                    # policy loss\n",
    "                    ratio = torch.exp(cur_log_probs - old_log_probs)  # same as cur_probs/old_probs, just with logs\n",
    "                    fst_surrogate = ratio * advantages_b\n",
    "                    snd_surrogate = torch.clip(ratio, 1 - clip_eps, 1 + clip_eps) * advantages_b\n",
    "                    policy_loss = -torch.min(fst_surrogate, snd_surrogate).mean() - entropy_bonus * entropy\n",
    "                    \n",
    "                    # value loss\n",
    "                    if clip_eps_value is not None:\n",
    "                        old_values_b = values[batch_indices]\n",
    "                        values_clipped = old_values_b + (values_b - old_values_b).clip(-clip_eps_value, clip_eps_value)\n",
    "                        fst_loss = F.huber_loss(values_clipped, returns_b, reduction='none')\n",
    "                        snd_loss = F.huber_loss(values_b, returns_b, reduction='none')\n",
    "                        value_loss = value_weight * torch.max(fst_loss, snd_loss).mean()  # take the more conservative one\n",
    "                    else:\n",
    "                        value_loss = value_weight * F.huber_loss(values_b, returns_b, reduction='mean')\n",
    "    \n",
    "                    cum_loss = policy_loss + value_loss\n",
    "                    optimizer.zero_grad()\n",
    "                    cum_loss.backward()\n",
    "                    global_l2_norm = nn.utils.clip_grad_norm_(network.parameters(), max_norm)\n",
    "                    optimizer.step()\n",
    "                    \n",
    "                    # Track statistics for this minibatch\n",
    "                    mb_policy_losses.append(policy_loss.item())\n",
    "                    mb_value_losses.append(value_loss.item())\n",
    "                    mb_entropies.append(entropy.item())\n",
    "                    mb_kls.append((old_log_probs - cur_log_probs).mean().item())\n",
    "                    clip_fraction = ((ratio > 1 + clip_eps) | (ratio < 1 - clip_eps)).float().mean().item()\n",
    "                    mb_clip_fractions.append(clip_fraction)\n",
    "                    mb_global_l2_norms.append(global_l2_norm)\n",
    "                    \n",
    "            # average stats across minibatches\n",
    "            avg_policy_loss = sum(mb_policy_losses) / len(mb_policy_losses)\n",
    "            avg_value_loss = sum(mb_value_losses) / len(mb_value_losses)\n",
    "            avg_entropy = sum(mb_entropies) / len(mb_entropies)\n",
    "            avg_kl = sum(mb_kls) / len(mb_kls)\n",
    "            avg_clip_fraction = sum(mb_clip_fractions) / len(mb_clip_fractions)\n",
    "            avg_global_l2_norm = sum(mb_global_l2_norms) / len(mb_global_l2_norms)\n",
    "            \n",
    "            history['step_count'] += actual_steps        \n",
    "            \n",
    "            update_time = time.time() - t0\n",
    "            wall_time = time.time() - t_start\n",
    "    \n",
    "            t0 = time.time()\n",
    "            with torch.no_grad():\n",
    "                exp_var = explained_variance(returns, values)\n",
    "            stats_time = time.time() - t0\n",
    "                \n",
    "            log = {\n",
    "                'entropy': avg_entropy,\n",
    "                'explained_variance': exp_var,\n",
    "                'loss_policy': avg_policy_loss,  # entropy bonus incl.\n",
    "                'loss_value': avg_value_loss,\n",
    "                'loss_entropy': -entropy_bonus * avg_entropy,\n",
    "                'loss_cum': avg_policy_loss + avg_value_loss,\n",
    "                'approx_kl': avg_kl,  # average across all minibatches\n",
    "                'clip_fraction': avg_clip_fraction,  # (policy ratio)\n",
    "                'global_l2_norm': avg_global_l2_norm,\n",
    "                'advantage': gae_advantages.mean().item(),  # pre standardization\n",
    "                'update': update,\n",
    "                'step_count': history['step_count'],\n",
    "                'approx_fps': actual_steps / (update_time + rollout_time),\n",
    "                'time_wall': wall_time,\n",
    "                'time_update': update_time,\n",
    "                'time_rollout': rollout_time,\n",
    "                'time_stats': stats_time,\n",
    "                'policy_lr': optimizer.param_groups[0]['lr'],\n",
    "                'entropy_bonus': entropy_bonus,\n",
    "            }\n",
    "    \n",
    "            if update % eval_freq == 0 or update == num_updates:\n",
    "                t0 = time.time()\n",
    "                eval_rewards_d = evaluate(network, stochastic=False)\n",
    "                eval_rewards_s = evaluate(network, stochastic=True)\n",
    "                eval_time = time.time() - t0\n",
    "                log.update({\n",
    "                    'eval_time': eval_time,\n",
    "                    'eval_rewards (deterministic)': eval_rewards_d,\n",
    "                    'eval_rewards (stochastic)': eval_rewards_s,\n",
    "                })\n",
    "    \n",
    "                clear_output(wait=True)\n",
    "                print(f'wall time {wall_time:.2f} s')\n",
    "                print(f'update {update}')\n",
    "                print(f'step {history[\"step_count\"]}')\n",
    "                print(f'eval rewards (deterministic) {eval_rewards_d:.5f}')\n",
    "                print(f'eval rewards (stochastic) {eval_rewards_s:.5f}')\n",
    "    \n",
    "            try: wandb.log(log, step=history['step_count'])\n",
    "            except: print(f'[warning] wandb log failed')\n",
    "                \n",
    "            for group in optimizer.param_groups:\n",
    "                group['lr'] *= lr_decay\n",
    "            entropy_bonus *= entropy_decay\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        save(network, history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0564c71d-1144-433d-ac13-1b74279648f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load(network, optimizer, history, './saved/breakout-10786461s.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e65b3c-4d07-44d4-854f-c2bc19ae9f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if KAGGLE:\n",
    "    import multiprocessing\n",
    "    multiprocessing.set_start_method('spawn', force=True)\n",
    "\n",
    "try: envs.reset(), envs.close()\n",
    "except: pass\n",
    "envs = make_envs(8)\n",
    "\n",
    "t0 = time.time()\n",
    "try:\n",
    "    with torch.device(device):\n",
    "        train(envs, 999999, history)\n",
    "except KeyboardInterrupt:\n",
    "        print('keyboard interrupt. stopping...')\n",
    "finally:\n",
    "    print(f'done.\\n*actual* time taken: {time.time() - t0:.2f} s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8028e771-3cc2-42f5-905e-0d3cbbe1e172",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-03-23T04:35:24.218207Z",
     "iopub.status.idle": "2025-03-23T04:35:24.218492Z",
     "shell.execute_reply": "2025-03-23T04:35:24.218378Z"
    }
   },
   "outputs": [],
   "source": [
    "evaluate(network, stochastic=True, render_mode='human', graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642ea9ee-bdd9-455b-a4b8-0c5113c10c01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c197c2c8-7df6-49e0-8012-fe27e84de10b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f745f1f-91b5-4593-a99f-ff78605eb2cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3460b06c-25e1-4043-bf5c-46858d583c20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8329e784-d0ea-4698-8d95-05b9fe0c7547",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d53d92-c5fc-4fa0-89d0-4c1aa932bc95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2e51f7-47a5-4b54-a088-f6700a3803d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5596398-4e04-41d7-8af9-d74e5d7f5de6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d3f812-02d6-4161-ab95-d7e11029e09a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631923ff-78bf-40ca-af64-607607ba442a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0199ce-bab1-4796-95a6-eba43120f802",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a21adf-a586-4853-8bb4-3e954ae82580",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5199ef-214a-406b-9e48-055d3437325c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bec9f9a-78ed-4060-889d-b40e66b053f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab94d87-154c-48d5-a94f-6f50b394f543",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "kaggleenv310",
   "language": "python",
   "name": "kaggle310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
